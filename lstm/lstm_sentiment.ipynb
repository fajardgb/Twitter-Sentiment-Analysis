{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK4hGtwYDM9x",
        "outputId": "878c65c0-3290-490a-9161-4357b8a9defe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Collecting keras\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 6.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires keras<2.10.0,>=2.9.0rc0, but you have keras 2.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.5.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 588.3 MB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 64.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.11.23-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, flatbuffers, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-22.11.23 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 77.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==3.8.3\n",
        "!pip install keras --upgrade\n",
        "!pip install pandas --upgrade\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "#transformers\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NeMm2b1DOoQ",
        "outputId": "a76831fe-eea0-4286-d26b-5445c4f54d96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size = 290419\n",
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "metadata": {
        "id": "c0zpGRIODS0I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoECcrwyDUzt",
        "outputId": "d13f4d19-a1e8-4387-d0f0-5b5b01c9a640"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_map = {0: \"NEGATIVE\", 2:\"NEUTRAL\", 4: \"POSITIVE\"}\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]"
      ],
      "metadata": {
        "id": "tht_a-bQJrqD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = str(text).lower().strip()\n",
        "    text = re.sub(\"\\n\", \"\", text)\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "fqDFUeI-qbKE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD POLITICIANS TWEETS\n",
        "main_dir = '/content/drive/MyDrive/nns/'\n",
        "twt_load_dir = main_dir+'twitter_api_data/original/'\n",
        "\n",
        "# tweets regarding the politicans\n",
        "mehmet_oz_df = pd.read_csv(twt_load_dir+\"adam_laxalt.csv\", encoding =DATASET_ENCODING)\n",
        "john_fetterman_df = pd.read_csv(twt_load_dir+\"john_fetterman.csv\")\n",
        "adam_laxalt_df = pd.read_csv(twt_load_dir+\"adam_laxalt.csv\")\n",
        "catherine_cortez_masto_df = pd.read_csv(twt_load_dir+\"catherine_cortez_masto.csv\")\n",
        "ron_johnson_df = pd.read_csv(twt_load_dir+\"ron_johnson.csv\")\n",
        "mandela_barnes_df = pd.read_csv(twt_load_dir+\"mandela_barnes.csv\")\n",
        "donald_bolduc_df = pd.read_csv(twt_load_dir+\"donald_bolduc.csv\")\n",
        "maggie_hassan_df = pd.read_csv(twt_load_dir+\"maggie_hassan.csv\")\n",
        "ted_budd_df = pd.read_csv(twt_load_dir+\"ted_budd.csv\")\n",
        "cheri_beasly_df = pd.read_csv(twt_load_dir+\"cheri_beasly.csv\")\n",
        "joe_pinion_df = pd.read_csv(twt_load_dir+\"joe_pinion.csv\")\n",
        "charles_schumer_df = pd.read_csv(twt_load_dir+\"charles_schumer.csv\")\n",
        "jd_vance_df = pd.read_csv(twt_load_dir+\"jd_vance.csv\")\n",
        "tim_ryan_df = pd.read_csv(twt_load_dir+\"tim_ryan.csv\")"
      ],
      "metadata": {
        "id": "RZEpR2YuEXtv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD LSTM MODEL\n",
        "main_dir = '/content/drive/MyDrive/nns/'\n",
        "nn_load_dir = main_dir+'saved_nn_models/'\n",
        "model = tf.keras.models.load_model(nn_load_dir+'LSTM.h5')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq09IbHXGj2h",
        "outputId": "86cc42d6-5ba2-4d56-83dc-6726883c90e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 300, 300)          9889500   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 300, 300)          0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256)               570368    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,460,125\n",
            "Trainable params: 570,625\n",
            "Non-trainable params: 9,889,500\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score, include_neutral=False):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return 0 if score < 0.5 else 1"
      ],
      "metadata": {
        "id": "PseWT6o3KZW7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD TOKENIZER\n",
        "tokenizer_path = main_dir+'tokenizer.pkl'\n",
        "\n",
        "with open(tokenizer_path, 'rb') as f:\n",
        "  tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "r3bGr9lgXuVd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, include_neutral=False):\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test], verbose=0)[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "    return label"
      ],
      "metadata": {
        "id": "nj4ZHh0E0rrQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(df):\n",
        "  df.Tweet = df.Tweet.apply(lambda x: preprocess(x))\n",
        "  df['sentiment'] = 0\n",
        "  length = len(df)\n",
        "  for i in range(length):\n",
        "    text = df.Tweet[i]\n",
        "    label = predict(text, include_neutral=False)\n",
        "    df.at[i, 'sentiment'] = label\n",
        "    "
      ],
      "metadata": {
        "id": "lV3quS_72fON"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run model on all csvs\n",
        "run_model(mehmet_oz_df)\n",
        "run_model(john_fetterman_df)\n",
        "run_model(adam_laxalt_df)\n",
        "run_model(catherine_cortez_masto_df)\n",
        "run_model(ron_johnson_df)\n",
        "run_model(mandela_barnes_df)\n",
        "run_model(donald_bolduc_df)\n",
        "run_model(maggie_hassan_df)\n",
        "run_model(ted_budd_df)\n",
        "run_model(cheri_beasly_df)\n",
        "run_model(joe_pinion_df)\n",
        "run_model(charles_schumer_df)\n",
        "run_model(jd_vance_df)\n",
        "run_model(tim_ryan_df)"
      ],
      "metadata": {
        "id": "N8YPBSYo3rSZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of positive and negatives tweets from each df\n",
        "def get_sentiment(df):\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if row['sentiment'] == 0:\n",
        "            neg += 1\n",
        "        else:\n",
        "            pos += 1\n",
        "    return pos, neg\n",
        "\n",
        "# get pos/neg ,pos/all, neg/all ratio for all dfs\n",
        "def get_ratios(df):\n",
        "    pos, neg = get_sentiment(df)\n",
        "    pos_all = pos / (pos + neg)\n",
        "    neg_all = neg / (pos + neg)\n",
        "    pos_neg = pos / neg\n",
        "    # round all ratios to 2 decimal places\n",
        "    pos_all = round(pos_all, 2)\n",
        "    neg_all = round(neg_all, 2)\n",
        "    pos_neg = round(pos_neg, 2)\n",
        "    return pos_all, neg_all, pos_neg\n",
        "\n",
        "# sum number of positive and negatives tweets from list of df\n",
        "def sum_sentiment(dfs):\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for df in dfs:\n",
        "        pos_df, neg_df = get_sentiment(df)\n",
        "        pos += pos_df\n",
        "        neg += neg_df\n",
        "    return pos, neg\n",
        "\n",
        "# average the ratios of winners and losers\n",
        "def avg_ratios(dfs):\n",
        "    pos_all = 0\n",
        "    neg_all = 0\n",
        "    pos_neg = 0\n",
        "    for df in dfs:\n",
        "        pos_all_df, neg_all_df, pos_neg_df = get_ratios(df)\n",
        "        pos_all += pos_all_df\n",
        "        neg_all += neg_all_df\n",
        "        pos_neg += pos_neg_df\n",
        "    pos_all = pos_all / len(dfs)\n",
        "    neg_all = neg_all / len(dfs)\n",
        "    pos_neg = pos_neg / len(dfs)\n",
        "    return pos_all, neg_all, pos_neg"
      ],
      "metadata": {
        "id": "MYsQK8Y06fIE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of positive and negatives tweets from each df\n",
        "pos, neg = get_sentiment(mehmet_oz_df)\n",
        "print(\"Mehmet Oz: \", pos, neg)\n",
        "pos, neg = get_sentiment(john_fetterman_df)\n",
        "print(\"John Fetterman: \", pos, neg)\n",
        "pos, neg = get_sentiment(adam_laxalt_df)\n",
        "print(\"Adam Laxalt: \", pos, neg)\n",
        "pos, neg = get_sentiment(catherine_cortez_masto_df)\n",
        "print(\"Catherine Cortez Masto: \", pos, neg)\n",
        "pos, neg = get_sentiment(ron_johnson_df)\n",
        "print(\"Ron Johnson: \", pos, neg)\n",
        "pos, neg = get_sentiment(mandela_barnes_df)\n",
        "print(\"Mandela Barnes: \", pos, neg)\n",
        "pos, neg = get_sentiment(donald_bolduc_df)\n",
        "print(\"Donald Bolduc: \", pos, neg)\n",
        "pos, neg = get_sentiment(maggie_hassan_df)\n",
        "print(\"Maggie Hassan: \", pos, neg)\n",
        "pos, neg = get_sentiment(ted_budd_df)\n",
        "print(\"Ted Budd: \", pos, neg)\n",
        "pos, neg = get_sentiment(cheri_beasly_df)\n",
        "print(\"Cheri Beasley: \", pos, neg)\n",
        "pos, neg = get_sentiment(joe_pinion_df)\n",
        "print(\"Joe Pinion: \", pos, neg)\n",
        "pos, neg = get_sentiment(charles_schumer_df)\n",
        "print(\"Charles Schumer: \", pos, neg)\n",
        "pos, neg = get_sentiment(jd_vance_df)\n",
        "print(\"JD Vance: \", pos, neg)\n",
        "pos, neg = get_sentiment(tim_ryan_df)\n",
        "print(\"Tim Ryan: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zIQoy5L6mAb",
        "outputId": "05b7e6cb-694f-4c33-d6ca-2d0438fc6934"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mehmet Oz:  115 105\n",
            "John Fetterman:  325 175\n",
            "Adam Laxalt:  115 105\n",
            "Catherine Cortez Masto:  50 80\n",
            "Ron Johnson:  480 20\n",
            "Mandela Barnes:  180 260\n",
            "Donald Bolduc:  10 15\n",
            "Maggie Hassan:  15 115\n",
            "Ted Budd:  55 445\n",
            "Cheri Beasley:  80 85\n",
            "Joe Pinion:  20 30\n",
            "Charles Schumer:  95 80\n",
            "JD Vance:  235 265\n",
            "Tim Ryan:  225 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get ratios of pos:neg tweets from all dfs\n",
        "pos_all, neg_all, pos_neg = get_ratios(mehmet_oz_df)\n",
        "print(\"Mehmet Oz: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(john_fetterman_df)\n",
        "print(\"John Fetterman: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(adam_laxalt_df)\n",
        "print(\"Adam Laxalt: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(catherine_cortez_masto_df)\n",
        "print(\"Catherine Cortez Masto: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(ron_johnson_df)\n",
        "print(\"Ron Johnson: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(mandela_barnes_df)\n",
        "print(\"Mandela Barnes: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(donald_bolduc_df)\n",
        "print(\"Donald Bolduc: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(maggie_hassan_df)\n",
        "print(\"Maggie Hassan: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(ted_budd_df)\n",
        "print(\"Ted Budd: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(cheri_beasly_df)\n",
        "print(\"Cheri Beasley: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(joe_pinion_df)\n",
        "print(\"Joe Pinion: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(charles_schumer_df)\n",
        "print(\"Charles Schumer: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(jd_vance_df)\n",
        "print(\"JD Vance: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(tim_ryan_df)\n",
        "print(\"Tim Ryan: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgqdn9WD6q9q",
        "outputId": "74ce4346-cb80-4a75-b783-b4026db13fd7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mehmet Oz:  0.52 0.48 1.1\n",
            "John Fetterman:  0.65 0.35 1.86\n",
            "Adam Laxalt:  0.52 0.48 1.1\n",
            "Catherine Cortez Masto:  0.38 0.62 0.62\n",
            "Ron Johnson:  0.96 0.04 24.0\n",
            "Mandela Barnes:  0.41 0.59 0.69\n",
            "Donald Bolduc:  0.4 0.6 0.67\n",
            "Maggie Hassan:  0.12 0.88 0.13\n",
            "Ted Budd:  0.11 0.89 0.12\n",
            "Cheri Beasley:  0.48 0.52 0.94\n",
            "Joe Pinion:  0.4 0.6 0.67\n",
            "Charles Schumer:  0.54 0.46 1.19\n",
            "JD Vance:  0.47 0.53 0.89\n",
            "Tim Ryan:  0.45 0.55 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# winners vs losers\n",
        "winners = [john_fetterman_df, catherine_cortez_masto_df, ron_johnson_df, maggie_hassan_df, ted_budd_df, charles_schumer_df, jd_vance_df]\n",
        "losers = [mehmet_oz_df, adam_laxalt_df, mandela_barnes_df, donald_bolduc_df, cheri_beasly_df, joe_pinion_df, tim_ryan_df]"
      ],
      "metadata": {
        "id": "PBCo_SQA61Ml"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sum number of pos and neg tweets from list of df\n",
        "pos, neg = sum_sentiment(winners)\n",
        "print(\"Winners: \", pos, neg)\n",
        "pos, neg = sum_sentiment(losers)\n",
        "print(\"Losers: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr9gHJKa66ou",
        "outputId": "a2561301-271c-4ac0-d8a4-08f75e745ff1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Winners:  1255 1180\n",
            "Losers:  745 875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# average the ratios of winners and losers\n",
        "pos_all, neg_all, pos_neg = avg_ratios(winners)\n",
        "print(\"Winners: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = avg_ratios(losers)\n",
        "print(\"Losers: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3-PnRQD7ENA",
        "outputId": "9b3eadd9-72e4-4ef6-cc4e-5363b1bc5830"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Winners:  0.46142857142857135 0.5385714285714286 4.115714285714286\n",
            "Losers:  0.4542857142857143 0.5457142857142857 0.8557142857142858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# democrats vs republicans\n",
        "dems = [mehmet_oz_df, adam_laxalt_df, ron_johnson_df, donald_bolduc_df, ted_budd_df, joe_pinion_df, jd_vance_df]\n",
        "reps = [john_fetterman_df, catherine_cortez_masto_df, mandela_barnes_df, maggie_hassan_df, cheri_beasly_df, charles_schumer_df, tim_ryan_df]\n"
      ],
      "metadata": {
        "id": "zwOI4WI47It0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sum numer of dem and rep tweets from list of df\n",
        "pos, neg = sum_sentiment(dems)\n",
        "print(\"Democrats: \", pos, neg)\n",
        "pos, neg = sum_sentiment(reps)\n",
        "print(\"Republicans: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOaIWlaD7NwS",
        "outputId": "34f22e0f-66c3-4ead-f9b8-14341f4b1080"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Democrats:  1030 985\n",
            "Republicans:  970 1070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#average the ratios of dems and reps\n",
        "pos_all, neg_all, pos_neg = avg_ratios(dems)\n",
        "print(\"Democrats: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = avg_ratios(reps)\n",
        "print(\"Republicans: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75oT4YO67UEc",
        "outputId": "94d64482-6152-4399-9b0a-372b2a2992d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Democrats:  0.4828571428571428 0.5171428571428571 4.078571428571429\n",
            "Republicans:  0.4328571428571429 0.5671428571428571 0.8928571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all dfs to csv\n",
        "path = main_dir+'politician_csvs/'\n",
        "john_fetterman_df.to_csv(path+\"john_fetterman_annotated.csv\")\n",
        "mehmet_oz_df.to_csv(path+\"mehmet_oz_annotated.csv\")\n",
        "adam_laxalt_df.to_csv(path+\"adam_laxalt_annotated.csv\")\n",
        "catherine_cortez_masto_df.to_csv(path+\"catherine_cortez_masto_annotated.csv\")\n",
        "ron_johnson_df.to_csv(path+\"ron_johnson_annotated.csv\")\n",
        "mandela_barnes_df.to_csv(path+\"mandela_barnes_annotated.csv\")\n",
        "donald_bolduc_df.to_csv(path+\"donald_bolduc_annotated.csv\")\n",
        "maggie_hassan_df.to_csv(path+\"maggie_hassan_annotated.csv\")\n",
        "ted_budd_df.to_csv(path+\"ted_budd_annotated.csv\")\n",
        "cheri_beasly_df.to_csv(path+\"cheri_beasly_annotated.csv\")\n",
        "joe_pinion_df.to_csv(path+\"joe_pinion_annotated.csv\")\n",
        "charles_schumer_df.to_csv(path+\"charles_schumer_annotated.csv\")\n",
        "jd_vance_df.to_csv(path+\"jd_vance_annotated.csv\")\n",
        "tim_ryan_df.to_csv(path+\"tim_ryan_annotated.csv\")"
      ],
      "metadata": {
        "id": "A2RC0lgo7alI"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}