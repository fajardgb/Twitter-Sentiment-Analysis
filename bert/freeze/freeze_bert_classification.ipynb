{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJjRbey-vm3B"
      },
      "outputs": [],
      "source": [
        "# Guide: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuOUQzxAvm3F",
        "outputId": "09838ceb-5c79-4619-b7c5-f386a7efaa04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "# finetuning bert language model for classification\n",
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNXlYl5b0zXR",
        "outputId": "e9ff04d3-89eb-4c74-c0b5-0704c7bc95a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.7/dist-packages (3.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install python-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEsBVml5095G",
        "outputId": "cc7d78d2-bff1-44bc-e621-c2b13e8c0b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.28.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZCXp338vm3H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbCUQQJNvm3H"
      },
      "outputs": [],
      "source": [
        "# load in dataset\n",
        "df = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", encoding = \"latin-1\", low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0urmr9fhvm3I"
      },
      "outputs": [],
      "source": [
        "# remove all columns except for the text and target\n",
        "df = df[['text', 'target']]\n",
        "df.columns = ['text', 'target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVR8IjTNvm3I"
      },
      "outputs": [],
      "source": [
        "# make all value 4 to 1 in target column\n",
        "df['target'] = df['target'].replace(4, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlSRfG6Wvm3J"
      },
      "outputs": [],
      "source": [
        "# split dataset into train, validation, and test\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['target'], random_state=2022, test_size=0.3, stratify=df['target'])\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, random_state=2022, test_size=0.5, stratify=temp_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fT8Rvlqvm3J",
        "outputId": "b1d2707b-b12d-4157-dae0-3834cc060a94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXl16J8zvm3K"
      },
      "outputs": [],
      "source": [
        "# sample data\n",
        "text = [\"Hi! This is a testing sample to see if this works. I hope it does!\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tStaq_nvm3L",
        "outputId": "a2fc0dac-e275-49ad-934b-120d625b1b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 7632, 999, 2023, 2003, 1037, 5604, 7099, 2000, 2156, 2065, 2023, 2573, 1012, 1045, 3246, 2009, 2515, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "# output\n",
        "print(sent_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "E4fsuoLbvm3M",
        "outputId": "41166c4e-2dcb-474a-8aa0-4948d7faca62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbc43cf8750>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# tokenization\n",
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGC0OjyWvm3M"
      },
      "outputs": [],
      "source": [
        "# define max length of sequence to be longest tweet in df\n",
        "max_seq_len = max(seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR5bh16Fvm3M",
        "outputId": "ac5252e8-b216-49ed-9c71-5b600cc89d5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd0qk1wRvm3N"
      },
      "outputs": [],
      "source": [
        "# Convert Integer Sequences to Tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCILP7Hfvm3N"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# define a batch size\n",
        "batch_size = 16\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udfcbolovm3O"
      },
      "outputs": [],
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMdxTTNhvm3O"
      },
      "outputs": [],
      "source": [
        "# define model class\n",
        "class BERT_Arch(nn.Module):\n",
        "    \n",
        "        def __init__(self, bert):\n",
        "            super(BERT_Arch, self).__init__()\n",
        "    \n",
        "            self.bert = bert\n",
        "    \n",
        "            # dropout layer\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "    \n",
        "            # relu activation function\n",
        "            self.relu =  nn.ReLU()\n",
        "    \n",
        "            # dense layer 1\n",
        "            self.fc1 = nn.Linear(768,512)\n",
        "    \n",
        "            # dense layer 2 (Output layer)\n",
        "            self.fc2 = nn.Linear(512,2)\n",
        "    \n",
        "            #softmax activation function\n",
        "            self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "        #define the forward pass\n",
        "        def forward(self, sent_id, mask):\n",
        "    \n",
        "            #pass the inputs to the model\n",
        "            _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "    \n",
        "            x = self.fc1(cls_hs)\n",
        "    \n",
        "            x = self.relu(x)\n",
        "    \n",
        "            x = self.dropout(x)\n",
        "    \n",
        "            # output layer\n",
        "            x = self.fc2(x)\n",
        "    \n",
        "            # apply softmax activation\n",
        "            x = self.softmax(x)\n",
        "    \n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7kMDPoTvm3P"
      },
      "outputs": [],
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71EhiZMrvm3P",
        "outputId": "9d379365-e964-4a35-b10b-86a2584f7fd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHmtBZMUvm3P",
        "outputId": "b9c69b2b-9c70-4815-f091-1eb8d647b6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 1.]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(class_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzN09plSvm3Q"
      },
      "outputs": [],
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcwrcIGKvm3Q"
      },
      "outputs": [],
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu1hA1yLvm3Q"
      },
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # # Calculate elapsed time in minutes.\n",
        "      # elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn94rA3uvm3R",
        "outputId": "28d9f0dd-134e-43bf-9f0a-748749c0fe90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.615\n",
            "Validation Loss: 0.649\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.572\n",
            "Validation Loss: 0.530\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.556\n",
            "Validation Loss: 0.511\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.552\n",
            "Validation Loss: 0.531\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.548\n",
            "Validation Loss: 0.556\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.544\n",
            "Validation Loss: 0.500\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.540\n",
            "Validation Loss: 0.510\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.539\n",
            "Validation Loss: 0.498\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.537\n",
            "Validation Loss: 0.506\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of  2,188.\n",
            "  Batch   100  of  2,188.\n",
            "  Batch   150  of  2,188.\n",
            "  Batch   200  of  2,188.\n",
            "  Batch   250  of  2,188.\n",
            "  Batch   300  of  2,188.\n",
            "  Batch   350  of  2,188.\n",
            "  Batch   400  of  2,188.\n",
            "  Batch   450  of  2,188.\n",
            "  Batch   500  of  2,188.\n",
            "  Batch   550  of  2,188.\n",
            "  Batch   600  of  2,188.\n",
            "  Batch   650  of  2,188.\n",
            "  Batch   700  of  2,188.\n",
            "  Batch   750  of  2,188.\n",
            "  Batch   800  of  2,188.\n",
            "  Batch   850  of  2,188.\n",
            "  Batch   900  of  2,188.\n",
            "  Batch   950  of  2,188.\n",
            "  Batch 1,000  of  2,188.\n",
            "  Batch 1,050  of  2,188.\n",
            "  Batch 1,100  of  2,188.\n",
            "  Batch 1,150  of  2,188.\n",
            "  Batch 1,200  of  2,188.\n",
            "  Batch 1,250  of  2,188.\n",
            "  Batch 1,300  of  2,188.\n",
            "  Batch 1,350  of  2,188.\n",
            "  Batch 1,400  of  2,188.\n",
            "  Batch 1,450  of  2,188.\n",
            "  Batch 1,500  of  2,188.\n",
            "  Batch 1,550  of  2,188.\n",
            "  Batch 1,600  of  2,188.\n",
            "  Batch 1,650  of  2,188.\n",
            "  Batch 1,700  of  2,188.\n",
            "  Batch 1,750  of  2,188.\n",
            "  Batch 1,800  of  2,188.\n",
            "  Batch 1,850  of  2,188.\n",
            "  Batch 1,900  of  2,188.\n",
            "  Batch 1,950  of  2,188.\n",
            "  Batch 2,000  of  2,188.\n",
            "  Batch 2,050  of  2,188.\n",
            "  Batch 2,100  of  2,188.\n",
            "  Batch 2,150  of  2,188.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.536\n",
            "Validation Loss: 0.501\n"
          ]
        }
      ],
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiLxRNFBvm3R",
        "outputId": "b847268d-bbe2-4167-c707-45b0dd05605a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load best model\n",
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-QSG8yFvm3R"
      },
      "outputs": [],
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWdJz35-vm3S",
        "outputId": "1be3fb15-47a6-466c-9b51-b6f091cdb6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.74      0.76      3750\n",
            "           1       0.75      0.78      0.77      3750\n",
            "\n",
            "    accuracy                           0.76      7500\n",
            "   macro avg       0.76      0.76      0.76      7500\n",
            "weighted avg       0.76      0.76      0.76      7500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "MBgfRGejvm3S",
        "outputId": "ced0b98c-c89f-4e73-c8ad-d1e83d20ccf8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-eb320b48-25d6-4cc1-9b65-b7a10201315c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2772</td>\n",
              "      <td>978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>808</td>\n",
              "      <td>2942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb320b48-25d6-4cc1-9b65-b7a10201315c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb320b48-25d6-4cc1-9b65-b7a10201315c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb320b48-25d6-4cc1-9b65-b7a10201315c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "col_0     0     1\n",
              "row_0            \n",
              "0      2772   978\n",
              "1       808  2942"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
